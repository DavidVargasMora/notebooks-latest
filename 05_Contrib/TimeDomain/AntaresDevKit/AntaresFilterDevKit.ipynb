{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Carl Stubens <cstubens@noao.edu>'\n",
    "__edited__ = 'Gautham Narayan <gnarayan@noao.edu>'\n",
    "__version__ = '20191017' # yyyymmdd\n",
    "__datasets__ = ['']\n",
    "__keywords__ = ['ANTARES', 'transient']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANTARES Filter Development Kit\n",
    "\n",
    "_Carl Stubens, Gautham Narayan, ANTARES Team._\n",
    "\n",
    "_Many thanks to Mike Fitzpatrick, Adam Scott, Knut Olsen, Jennifer Andrews, Robert Nikutta._\n",
    "\n",
    "## Summary\n",
    "\n",
    "This Notebook demonstrates how to write filters for [ANTARES](http://antares.noao.edu) and test them against a sample of real data from [ZTF](http://ztf.caltech.edu/).\n",
    "\n",
    "This Notebook is intended to be used in NOAO DataLab's Jupyter environment. There, you will have access to ANTARES test data. If you're not running in DataLab, [sign up for DataLab](https://datalab.noao.edu), then [log in to the notebook server](https://datalab.noao.edu/devbooks).\n",
    "\n",
    "For new Data Lab accounts, this notebook will be automatically included in your `notebooks/` directory. Otherwise, you can save this `.ipynb` notebook file locally, and then upload it to your Data Lab Jupyter notebook server (use the 'Upload' button in the upper right corner).\n",
    "\n",
    "In Data Lab, you MUST use the Kernel version \"Python 3 (ANTARES)\".\n",
    "## Goals\n",
    "\n",
    "To demonstrate:\n",
    "\n",
    "1. How to write filters using the ANTARES filter API.\n",
    "1. How to test filters against a small test dataset.\n",
    "\n",
    "Note: As of this writing, the test dataset is limited. It is intended to represent the format of ZTF data in ANTARES' format and API. It is not intended to represent the variety of data that is available, or to be suitable for training machine learning systems.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [0. Background information on ANTARES](#background)\n",
    "* [1. Connect to test database](#connect)\n",
    "* [2. Write a Filter](#write)\n",
    "* [3. Test a Filter](#test)\n",
    " * [3.1 Test against an Alert from test dataset](#test-one)\n",
    " * [3.2 Test against an Alert from the dataset using catalog information](#test-two)\n",
    " * [3.3 Test against an Alert from the dataset using catalog and alert information](#test-three)\n",
    " * [3.4 Test against an user-created alert using catalog and alert information](#test-four)\n",
    "* [4. Submit Filter to ANTARES](#submit)\n",
    "\n",
    "<a class=\"anchor\" id=\"background\"></a>\n",
    "## 0. Background information on ANTARES\n",
    "\n",
    "ANTARES receives alerts from surveys in real-time and sends them through a processing pipeline. The pipeline contains the following stages:\n",
    "\n",
    "1. Associate the Alert with the nearest point of known past measurements within a 1\" radius. We call this a Locus.\n",
    "2. Discard Alerts with a high probability of being false detections.\n",
    "3. Discard Alerts with poor image quality.\n",
    "4. Look up associated objects in our catalogs.\n",
    "5. If the Alert's Locus has two or more measurements on it, execute the Filters.\n",
    "\n",
    "The filters are python functions which take a LocusData object as a single parameter. Functions on the LocusData provide access to the Alert's properties, the data from past Alerts on the Locus, and the associated catalog objects. The LocusData also provides functions to set new properties on the Alert, and to send it to output streams.\n",
    "\n",
    "<a class=\"anchor\" id=\"connect\"></a>\n",
    "## 1. Initalize the dev kit\n",
    "\n",
    "This will configure the `antares` package to connect to the test database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import antares.dev_kit as dk\n",
    "dk.init()\n",
    "# You should see a happy message that says that \"ANTARES DevKit is ready!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"write\"></a>\n",
    "## 2. Write a Filter\n",
    "\n",
    "The filter `example_filter` below does nothing of scientific interest, but it demonstrates the most basic use of the filter API.\n",
    "\n",
    "Further down, there are other example filters -  `high_snr`, `extragalactic`, etc. are examples of our current science filters. Each demonstrates one new feature of the API that you might want to build into your own super awesome science filter.\n",
    "\n",
    "The Filter API consists of the LocusData object, which is passed to the Filter as the single parameter. The `example_filter` shows examples of how to use the LocusData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_filter(locus_data):\n",
    "    \"\"\"\n",
    "    A test Filter for demonstration.\n",
    "    \"\"\"\n",
    "    print('`example_filter` is running...')\n",
    "\n",
    "    # Get a dict of all properties on the new alert.\n",
    "    print('locus_data.get_properties()')\n",
    "    print('-->')\n",
    "    print(locus_data.get_properties())\n",
    "    print(\"\\nYikes! You can see there are lots of properties!\")\n",
    "\n",
    "    # Any properties from the ZTF Alert are prefixed with 'ztf_'.\n",
    "    # See here for ZTF's documentation of their properties:\n",
    "    # https://github.com/ZwickyTransientFacility/ztf-avro-alert/blob/master/schema/candidate.avsc\n",
    "\n",
    "    # Get a numpy array of values for particular properties.\n",
    "    # Rows for 'alert_id' and 'mjd' are always included at the top of the array.\n",
    "    # For example, in the following examples, the rows of the array will be:\n",
    "    # - alert_id\n",
    "    # - mjd\n",
    "    # - ra\n",
    "    # - dec\n",
    "    # - ztf_fid (1=g, 2=r, 3=i)\n",
    "    # - ztf_magpsf\n",
    "    print(\"Most times, it's easier to work with the light curve which you can do with the get_time_series function\")\n",
    "    print(\"locus_data.get_time_series('ra', 'dec', 'ztf_fid', 'ztf_magpsf')\")\n",
    "    print('-->')\n",
    "    print(locus_data.get_time_series('ra', 'dec', 'ztf_fid', 'ztf_magpsf'))\n",
    "    print(\"\\nBy default, the function returns alerts in the filter/passband that match the current alert's passband.\")\n",
    "    print(\"You can request a specific filter instead.\\n\")\n",
    "    # In the following example, we specify only to include columns where ztf_fid == 2.\n",
    "    print(\"locus_data.get_time_series('ra', 'dec', 'ztf_fid', 'ztf_magpsf', filters={'ztf_fid': 2})\")\n",
    "    print('-->')\n",
    "    print(locus_data.get_time_series('ra', 'dec', 'ztf_fid', 'ztf_magpsf', filters={'ztf_fid': 2}))\n",
    "\n",
    "    # Send the Alert to an output stream.\n",
    "    # The name of your stream must be unique. We will check this before accepting your filter.\n",
    "    # All streams are directed to Kafka output topics with the same name as the stream.\n",
    "    # We can also configure your stream to send notifications to a channel in Slack.\n",
    "    locus_data.send_to_stream('my_stream')\n",
    "\n",
    "    print('`example_filter` is finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"test\"></a>\n",
    "## 3. Test a filter\n",
    "\n",
    "<a class=\"anchor\" id=\"test-one\"></a>\n",
    "### 3.1 Test against an Alert from test dataset\n",
    "\n",
    "We have placed a sample of the ANTARES database (sourced from ZTF) in a read-only database for testing.\n",
    "\n",
    "Here, we run the `example_filter` against a particular Alert and its measurement history. The `run_stage` function takes an Alert ID and a filter, and runs the filter by constructing a LocusData object identical to what would be generated in the ANTARES production system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_id = 153505\n",
    "\n",
    "# Run the `example_filter`.\n",
    "# `verbose=True` prints detailed logs.\n",
    "result = dk.run_stage(alert_id, example_filter, verbose=True)\n",
    "\n",
    "# `run_stage` returns a dict with a report of what happened:\n",
    "print(list(result.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look at what streams were created or populated by this alert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result['new_streams'])\n",
    "\n",
    "# You can get the LocusData object too, so that you can work with it outside the stage\n",
    "ld = result['locus_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"test-two\"></a>\n",
    "### 3.2 Test against an Alert from test dataset, using catalog information\n",
    "\n",
    "Let's create a more complex filter and use the alert's catalog information to make a decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extragalactic(ld):\n",
    "    \"\"\"\n",
    "    Send alert to stream 'extragalactic' if it matches any extended source catalogs.\n",
    "    \"\"\"\n",
    "    matching_catalog_names = ld.get_astro_object_matches().keys()\n",
    "\n",
    "    # These are the catalogs (Antares-based names) with extended sources\n",
    "    xsc_cats = ['2mass_xsc', 'ned', 'nyu_valueadded_gals', 'sdss_gals', 'veron_agn_qso']\n",
    "\n",
    "    if set(matching_catalog_names) & set(xsc_cats):\n",
    "        p = ld.get_properties()\n",
    "        print(p['alert_id'], ' matches an extragalactic source')\n",
    "        ld.send_to_stream('extragalactic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results   = dk.run_many(extragalactic, n=50, verbose=False) # notice we're running 50 alerts at once here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"test-three\"></a>\n",
    "### 3.3 Test against an Alert from test dataset, using catalog information AND alert properties to make a decision\n",
    "\n",
    "Let's create a more complex filter, and this time, also use the alert's properties together with catalog information to make a decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_snr(ld):\n",
    "    \"\"\"\n",
    "    Send high-SNR non-variable star alerts to stream 'high_snr'.\n",
    "\n",
    "    Should flag ~2-3% of alerts.\n",
    "    \"\"\" \n",
    "    snr_thresholds = {\n",
    "        1: 50.0,  # For filter ID 1 (g), the threshold is 50\n",
    "        2: 55.0,  # For filter ID 2 (R), the threshold is 55\n",
    "    }\n",
    "    \n",
    "    # we'll check that a star is in a variable star catalog\n",
    "    bad_cats = set(['asassn_variable_catalog', 'asassn_variable_catalog_v2_20190802'])\n",
    "\n",
    "    # here's an example of how to get the astro object information\n",
    "    astro_objects = ld.get_astro_object_matches()\n",
    "    cats = set(astro_objects.keys())\n",
    "    n_cats = len(cats)\n",
    "\n",
    "    p = ld.get_properties()  # get all Alert properties as a dict\n",
    "    fid = p['ztf_fid']  # filter ID\n",
    "     \n",
    "    sigmapsf = p.get('ztf_sigmapsf', None)  # 1-sigma uncertainty in magnitude of PSF \n",
    "    if sigmapsf is None: # only available for detections\n",
    "        return\n",
    "    \n",
    "    snr = 1.0 / sigmapsf  # compute SNR\n",
    "    snr_threshold = snr_thresholds.get(fid, None)  # SNR threshold for this filter\n",
    "    \n",
    "    # note that the catalog check doesn't guarantee the alert isn't from a variable star\n",
    "    # only that the object it is coming from isn't in a variable star catalog we have\n",
    "    # of course, most of the bright variable stars that have any chance of exceeding the S/N threshold\n",
    "    # are already in the catalogs, so this is reasonable\n",
    "    # what we want is for this filter to trip on new flares \n",
    "    if snr_threshold is not None and snr > snr_threshold:\n",
    "        if cats.isdisjoint(bad_cats):\n",
    "            print(\"Alert ID: \", p['alert_id'], \"    SNR: \",snr,\"    n:\",n_cats)\n",
    "            if n_cats > 0:\n",
    "                print(list(astro_objects.keys()))\n",
    "            ld.send_to_stream('high_snr')\n",
    "        else:\n",
    "            print('Alert ID ',p['alert_id'],' matched with known variable star.')\n",
    "            bad_objs = [astro_objects[bad_cat] for bad_cat in bad_cats]\n",
    "            for bad_obj in bad_objs:\n",
    "                print(bad_obj)\n",
    "        print(\"#####\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, lets process a bunch of alerts at once. We'll put one alert (id=152476) in that we know will trip this filter, and one that could have, except is in a variable star catalog (id=31544)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_ids = sorted(set(dk.get_alert_ids(5) + [152476, 31544 ])) # notice here we're getting \n",
    "# 5 random alerts and adding two to the list\n",
    "results   = dk.run_many(high_snr, alert_ids=alert_ids, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"test-four\"></a>\n",
    "### 3.4 Test against a user-created Alert, using catalog information AND alert properties to make a decision\n",
    "\n",
    "Here's a little more realistic of an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuclear_transient(ld):\n",
    "    \"\"\"\n",
    "    Send alert to stream 'Nuclear Transient' if it is within 0.6 arcseconds of a\n",
    "    source in the ZTF reference frame. It is also required that a match within\n",
    "    1\" of a known Pan-STARRS galaxy (ztf_distpsnr1 < 1. and ztf_sgscore1<0.3).\n",
    "    To further remove small flux fluctuaion, we also require magpsf (alert PSF\n",
    "    photometry) - magnr (PSF photometry of the nearby source in the reference\n",
    "    image) > 1.5. The selection criteria are from Sjoert van Velzen et al.\n",
    "    (2018, arXiv:1809.02608), section 2.1.\n",
    "    \"\"\"\n",
    "    p = ld.get_properties()\n",
    "    \n",
    "    alert_id = p['alert_id']\n",
    "    sgscore  = p.get('ztf_sgscore1', None)\n",
    "    distpsnr = p.get('ztf_distpsnr1', None)\n",
    "    magpsf   = p.get('ztf_magpsf', None)\n",
    "    magnr    = p.get('ztf_magnr', None)\n",
    "    distnr   = p.get('ztf_distnr', None)\n",
    "\n",
    "    if None in (distnr, distpsnr, sgscore, magpsf, magnr):\n",
    "        return\n",
    "    \n",
    "    \n",
    "    if distnr < 0.6 and distpsnr < 1. and sgscore < 0.3 and magpsf - magnr < 1.5:\n",
    "        \n",
    "        matching_catalog_names = ld.get_astro_object_matches().keys()\n",
    "        \n",
    "        if 'veron_aqn_qso' in matching_catalog_names:\n",
    "            print(alert_id,\" AGN activity\")\n",
    "            ld.send_to_stream(\"agn_qso_activity\")\n",
    "        elif 'french_post_starburst_gals' in matching_catalog_names:\n",
    "            print(alert_id,\"TDE candidate\")\n",
    "            ld.send_to_stream(\"tde_candidate\")\n",
    "        else:\n",
    "            print(\"Nuclear activity\")\n",
    "            ld.send_to_stream(\"nuclear_transient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a more complex filter. It may not trigger, even if we give it a random set of 20 alerts - most won't be nuclear. We could make it 500 and there's still a good chance it'll not trigger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results   = dk.run_many(nuclear_transient, n=20, verbose=False)\n",
    "# even if it doesn't trigger you can still see timing information from run_many\n",
    "print({key: results[key] for key in results.keys() if key.startswith('t_')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# to deal with this sort of thing, you can also create an alert of your own designed to trigger (or not)\n",
    "# this is a good way to validate your code's behavior with known good (or bad) input\n",
    "\n",
    "alerts = [{'ztf_distnr':0.5, 'ztf_distpsnr1':0.7, 'ztf_sgscore1':0.01, 'ztf_magpsf':19.5, 'ztf_magnr':19.3},]\n",
    "fake_alert_id = dk.mock_locus(10.704, 41.269, alerts)\n",
    "dk.run_stage(fake_alert_id, nuclear_transient, verbose=False)\n",
    "# this should trigger the stage to show \"Nuclear activity\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also get the LocusData object directly for tinkering with.\n",
    "\n",
    "alert_id = 153505\n",
    "ld = dk.get_locus_data(alert_id)\n",
    "\n",
    "print(ld)\n",
    "print(sorted(ld.get_properties().keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_m31(ld):\n",
    "    \"\"\"\n",
    "    Send alerts to stream 'in_m31' if Alert is within a 2-square-degree box\n",
    "    centered on M31.\n",
    "    \"\"\"\n",
    "    ra_max = 11.434793\n",
    "    ra_min = 9.934793\n",
    "    dec_max = 42.269065\n",
    "    dec_min = 40.269065\n",
    "\n",
    "    p = ld.get_properties()\n",
    "    ra = p['ra']\n",
    "    dec = p['dec']\n",
    "\n",
    "    if ra_max > ra > ra_min \\\n",
    "    and dec_max > dec > dec_min:\n",
    "        print('In M31')\n",
    "        ld.send_to_stream(\"in_m31\")\n",
    "    else:\n",
    "        print('Not in M31')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_m31(ld)\n",
    "\n",
    "# we can also use the simulated locus we created earlier to test this stage:\n",
    "fake_locus = dk.get_locus_data(fake_alert_id)\n",
    "in_m31(fake_locus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"submit\"></a>\n",
    "## 4. Submit Filter to ANTARES\n",
    "\n",
    "When you're ready to submit your filter to ANTARES, copy your filter function definition into the form on the ANTARES website at:\n",
    "\n",
    "* http://antares.noao.edu/filters\n",
    "\n",
    "You will need to provide:\n",
    "\n",
    "* Your filter function, helper functions, and `import` statements as a single block of code.\n",
    "\n",
    "* A unique name for your filter.\n",
    "\n",
    "* A brief text description of your filter.\n",
    "\n",
    "* The \"handler\", which is the name of the filter function in your code. This determines which function ANTARES will call. The handler name does not need to be unique outside of your code. The handler function must accept a single parameter, which is the `LocusData` object. You may name the parameter anything you like. We reccomend `locus_data` or `ld`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ANTARES)",
   "language": "python",
   "name": "antares_py3.7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
