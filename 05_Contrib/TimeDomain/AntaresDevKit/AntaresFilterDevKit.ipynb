{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = 'Carl Stubens <cstubens@noao.edu>'\n",
    "__version__ = '20190731' # yyyymmdd\n",
    "__datasets__ = ['']\n",
    "__keywords__ = ['ANTARES', 'transient']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANTARES Filter Development Kit\n",
    "\n",
    "_Carl Stubens, ANTARES Team._\n",
    "\n",
    "_Many thanks to Mike Fitzpatrick, Adam Scott, Knut Olsen, Jennifer Andrews, Robert Nikutta._\n",
    "\n",
    "## Summary\n",
    "\n",
    "This Notebook demonstrates how to write filters for [ANTARES](http://antares.noao.edu) and test them against a sample of real data from [ZTF](http://ztf.caltech.edu/).\n",
    "\n",
    "This Notebook is intended to be used in NOAO DataLab's Jupyter environment. There, you will have access to ANTARES test data. If you're not running in DataLab, [sign up for DataLab](https://datalab.noao.edu), then [log in to the notebook server](https://datalab.noao.edu/devbooks).\n",
    "\n",
    "For new Data Lab accounts, this notebook will be automatically included in your `notebooks/` directory. Otherwise, you can save this `.ipynb` notebook file locally, and then upload it to your Data Lab Jupyter notebook server (use the 'Upload' button in the upper right corner).\n",
    "\n",
    "## Goals\n",
    "\n",
    "To demonstrate:\n",
    "\n",
    "1. How to write filters using the ANTARES filter API.\n",
    "1. How to test filters against a small test dataset.\n",
    "\n",
    "Note: As of this writing, the test dataset is limited. It is intended to represent the format of ZTF data in ANTARES' format and API. It is not intended to represent the variety of data that is available, or to be suitable for training machine learning systems.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* [0. Background information on ANTARES](#background)\n",
    "* [1. Connect to test database](#connect)\n",
    "* [2. Write a Filter](#write)\n",
    "* [3. Test a Filter](#test)\n",
    " * [3.1 Test against an Alert from test dataset](#test-one)\n",
    " * [3.2 Test against multiple Alerts](#test-many)\n",
    "* [4. Submit Filter to ANTARES](#submit)\n",
    "\n",
    "<a class=\"anchor\" id=\"background\"></a>\n",
    "## 0. Background information on ANTARES\n",
    "\n",
    "ANTARES receives alerts from surveys in real-time and sends them through a processing pipeline. The pipeline contains the following stages:\n",
    "\n",
    "1. Associate the Alert with the nearest point of known past measurements within a 1\" radius. We call this a Locus.\n",
    "2. Discard Alerts with a high probability of being false detections.\n",
    "3. Discard Alerts with poor image quality.\n",
    "4. Look up associated objects in our catalogs.\n",
    "5. If the Alert's Locus has two or more measurements on it, execute the Filters.\n",
    "\n",
    "The filters are python functions which take a LocusData object as a single parameter. Functions on the LocusData provide access to the Alert's properties, the data from past Alerts on the Locus, and the associated catalog objects. The LocusData also provides functions to set new properties on the Alert, and to send it to output streams.\n",
    "\n",
    "<a class=\"anchor\" id=\"connect\"></a>\n",
    "## 1. Connect to test database\n",
    "\n",
    "First, we configure the `antares` package to connect to the test database, and we test the database connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import antares\n",
    "from antares.config import config\n",
    "from antares.database import engine\n",
    "from antares.dev_kit.run_stage import run_stage, get_locus_data\n",
    "from antares.dev_kit.fetch_data import get_alert_ids\n",
    "print('Using ANTARES version', antares.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure connections to the test database.\n",
    "config.ALERT_DATABASE_URL = \"mysql://antares_datalab:pro_Moonrise_epi@antdb01.dm.noao.edu:3306/antares_devkit\"\n",
    "config.CATALOG_DATABASE_URL = \"mysql://antares_datalab:pro_Moonrise_epi@antdb01.dm.noao.edu:3308/astro_catalog\"\n",
    "engine.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test database connections\n",
    "print('Connecting to Alert DB...')\n",
    "print(engine.get_alert_db_engine().execute('SELECT \"OK\"').scalar())\n",
    "print('Connecting to Catalog DB...')\n",
    "print(engine.get_catalog_db_engine().execute('SELECT \"OK\"').scalar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"write\"></a>\n",
    "## 2. Write a Filter\n",
    "\n",
    "The filter `example_filter` below does nothing of scientific interest, but it demonstrates the use of the filter API.\n",
    "\n",
    "Further down, the filter `high_snr`, `extragalactic`, etc. are examples of our current science filters.\n",
    "\n",
    "The Filter API consists of the LocusData object, which is passed to the Filter as the single parameter. The `example_filter` shows examples of how to use the LocusData."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_filter(locus_data):\n",
    "    \"\"\"\n",
    "    A test Filter for demonstration.\n",
    "    \"\"\"\n",
    "    print('`example_filter` is running...')\n",
    "\n",
    "    # Get a dict of all properties on the new alert.\n",
    "    print('locus_data.get_properties()')\n",
    "    print('-->')\n",
    "    print(locus_data.get_properties())\n",
    "    print()\n",
    "\n",
    "    # Any properties from the ZTF Alert are prefixed with 'ztf_'.\n",
    "    # See here for ZTF's documentation of their properties:\n",
    "    # https://github.com/ZwickyTransientFacility/ztf-avro-alert/blob/master/schema/candidate.avsc\n",
    "\n",
    "    # Get a numpy array of values for particular properties.\n",
    "    # Rows for 'alert_id' and 'mjd' are always included at the top of the array.\n",
    "    # For example, in the following examples, the rows of the array will be:\n",
    "    # - alert_id\n",
    "    # - mjd\n",
    "    # - ra\n",
    "    # - dec\n",
    "    # - ztf_fid\n",
    "    # - ztf_magpsf\n",
    "    print(\"locus_data.get_time_series('ra', 'dec', 'ztf_fid', 'ztf_magpsf')\")\n",
    "    print('-->')\n",
    "    print(locus_data.get_time_series('ra', 'dec', 'ztf_fid', 'ztf_magpsf'))\n",
    "    print()\n",
    "    # In the following example, we specify only to include columns where ztf_fid == 2.\n",
    "    print(\"locus_data.get_time_series('ra', 'dec', 'ztf_fid', 'ztf_magpsf', filters={'ztf_fid': 2})\")\n",
    "    print('-->')\n",
    "    print(locus_data.get_time_series('ra', 'dec', 'ztf_fid', 'ztf_magpsf', filters={'ztf_fid': 2}))\n",
    "\n",
    "    # get_astro_object_matches() returns a datastructure like so:\n",
    "    # {catalog_name1: [match1, match2, ...],\n",
    "    #  catalog_name2: [...],\n",
    "    #  ...}\n",
    "    print()\n",
    "    print('locus_data.get_astro_object_matches()')\n",
    "    print('-->')\n",
    "    astro_objects = locus_data.get_astro_object_matches()\n",
    "    print(astro_objects)\n",
    "    print()\n",
    "    print('found catalog matches from catalogs: {}'.format(list(astro_objects.keys())))\n",
    "    for catalog_name, objects in astro_objects.items():\n",
    "        print()\n",
    "        print(catalog_name)\n",
    "        for obj in objects:\n",
    "            print(obj)\n",
    "\n",
    "    # Set some new properties on this Alert.\n",
    "    # Any properties that you create in this way will be stored and visible on the ANTARES website.\n",
    "    # The properties will also be included in the Kafka output messages.\n",
    "    # Properties may be of type `int`, `float`, or `str`.\n",
    "    locus_data.set_property('x', 500)\n",
    "    locus_data.set_property('y', 3.14)\n",
    "    locus_data.set_property('z', 'hello')\n",
    "\n",
    "    # Send the Alert to an output stream.\n",
    "    # The name of your stream must be unique. We will check this before accepting your filter.\n",
    "    # All streams are directed to Kafka output topics with the same name as the stream.\n",
    "    # We can also configure your stream to send notifications to a channel in Slack.\n",
    "    locus_data.send_to_stream('my_stream')\n",
    "\n",
    "    print('`example_filter` is finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are examples of real ANTARES filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def high_snr(ld):\n",
    "    \"\"\"\n",
    "    Send high-SNR alerts to stream 'high_snr'.\n",
    "\n",
    "    Should flag ~2-3% of alerts.\n",
    "    \"\"\"\n",
    "    snr_thresholds = {\n",
    "        1: 50.0,  # For filter ID 1 (g), the threshold is 50\n",
    "        2: 55.0,  # For filter ID 2 (R), the threshold is 55\n",
    "    }\n",
    "\n",
    "    p = ld.get_properties()  # get all Alert properties as a dict\n",
    "    fid = p['ztf_fid']  # filter ID\n",
    "    sigmapsf = p['ztf_sigmapsf']  # 1-sigma uncertainty in magnitude of PSF \n",
    "    snr = 1.0 / sigmapsf  # compute SNR\n",
    "    snr_threshold = snr_thresholds.get(fid, None)  # SNR threshold for this field\n",
    "\n",
    "    if snr_threshold is not None and snr > snr_threshold:\n",
    "        ld.send_to_stream('high_snr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extragalactic(ld):\n",
    "    \"\"\"\n",
    "    Send alert to stream 'extragalactic' if it matches any extended source catalogs.\n",
    "    \"\"\"\n",
    "    matching_catalog_names = ld.get_astro_object_matches().keys()\n",
    "\n",
    "    # These are the catalogs (Antares-based names) with extended sources\n",
    "    xsc_cats = ['2mass_xsc', 'ned', 'nyu_valueadded_gals', 'sdss_gals', 'veron_agn_qso']\n",
    "\n",
    "    if set(matching_catalog_names) & set(xsc_cats):\n",
    "        ld.send_to_stream('extragalactic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nuclear_transient(ld):\n",
    "    \"\"\"\n",
    "    Send alert to stream 'Nuclear Transient' if it is within 0.6 arcseconds of a\n",
    "    source in the ZTF reference frame. It is also required that a match within\n",
    "    1\" of a known Pan-STARRS galaxy (ztf_distpsnr1 < 1. and ztf_sgscore1<0.3).\n",
    "    To further remove small flux fluctuaion, we also require magpsf (alert PSF\n",
    "    photometry) - magnr (PSF photometry of the nearby source in the reference\n",
    "    image) > 1.5. The selection criteria are from Sjoert van Velzen et al.\n",
    "    (2018, arXiv:1809.02608), section 2.1.\n",
    "    \"\"\"\n",
    "    p = ld.get_properties()\n",
    "    sgscore = p['ztf_sgscore1']\n",
    "    distpsnr = p['ztf_distpsnr1']\n",
    "    magpsf = p['ztf_magpsf']\n",
    "    magnr = p['ztf_magnr']\n",
    "    distnr = p['ztf_distnr']\n",
    "\n",
    "    if None in (distnr, distpsnr, sgscore, magpsf, magnr):\n",
    "        return\n",
    "    \n",
    "    if distnr < 0.6 and distpsnr < 1. and sgscore < 0.3 and magpsf - magnr < 1.5:\n",
    "        ld.send_to_stream(\"nuclear_transient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_m31(ld):\n",
    "    \"\"\"\n",
    "    Send alerts to stream 'in_m31' if Alert is within a 2-square-degree box\n",
    "    centered on M31.\n",
    "    \"\"\"\n",
    "    ra_max = 11.434793\n",
    "    ra_min = 9.934793\n",
    "    dec_max = 42.269065\n",
    "    dec_min = 40.269065\n",
    "\n",
    "    p = ld.get_properties()\n",
    "    ra = p['ra']\n",
    "    dec = p['dec']\n",
    "\n",
    "    if ra_max > ra > ra_min \\\n",
    "    and dec_max > dec > dec_min:\n",
    "        ld.send_to_stream(\"in_m31\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"test\"></a>\n",
    "## 3. Test a filter\n",
    "\n",
    "<a class=\"anchor\" id=\"test-one\"></a>\n",
    "### 3.1 Test against an Alert from test dataset\n",
    "\n",
    "We have placed a sample of the ANTARES database (sourced from ZTF) in a read-only database for testing.\n",
    "\n",
    "Here, we run the `example_filter` against a particular Alert and its measurement history. The `run_stage` function takes an Alert ID and a filter, and runs the filter by constructing a LocusData object identical to what would be generated in the ANTARES production system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_id = 153505\n",
    "\n",
    "# Run the `example_filter`.\n",
    "# `verbose=True` prints detailed logs.\n",
    "result = run_stage(alert_id, example_filter, verbose=True)\n",
    "\n",
    "# `run_stage` returns a dict with a report of what happened:\n",
    "print()\n",
    "print(list(result.keys()))\n",
    "print(result['new_properties'])\n",
    "print(result['new_streams'])\n",
    "\n",
    "# You can get the LocusData object too:\n",
    "ld = result['locus_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"test-many\"></a>\n",
    "### 3.2 Test against multiple Alerts\n",
    "\n",
    "We can also run the filter against multiple Alerts from the database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch `n` Alert IDs from the test database\n",
    "\n",
    "def run_many(f, n=10):\n",
    "    return [run_stage(alert_id, f)\n",
    "            for alert_id in get_alert_ids(n)]\n",
    "\n",
    "results = run_many(example_filter, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also get the LocusData object directly for tinkering with.\n",
    "\n",
    "alert_id = 153505\n",
    "ld = get_locus_data(alert_id)\n",
    "\n",
    "print(ld)\n",
    "print(sorted(ld.get_properties().keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"submit\"></a>\n",
    "## 4. Submit Filter to ANTARES\n",
    "\n",
    "When you're ready to submit your filter to ANTARES, copy your filter function definition into the form on the ANTARES website at:\n",
    "\n",
    "* http://antares.noao.edu/filters\n",
    "\n",
    "You will need to provide:\n",
    "\n",
    "* Your filter function, helper functions, and `import` statements as a single block of code.\n",
    "\n",
    "* A unique name for your filter.\n",
    "\n",
    "* A brief text description of your filter.\n",
    "\n",
    "* The \"handler\", which is the name of the filter function in your code. This determines which function ANTARES will call. The handler name does not need to be unique outside of your code. The handler function must accept a single parameter, which is the `LocusData` object. You may name the parameter anything you like. We reccomend `locus_data` or `ld`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
